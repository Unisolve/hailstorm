# Orca: Progressive Learning from Complex Explanation Traces of GPT-4

## Our summary

Introduces Orca, a 13-billion parameter model that learns to imitate the reasoningi
process of LFMs (Large Foundational Models), specifically GPT 4.

Other approaches use LFMs "as teachers to generate large datasets, for instruction
tuning, and to train smaller models, such as Alpaca, WizardLM and Vicuna. While
these models can produce content that matches the style of their teachers, they
often fall short in terms of the reasoning and comprehension skills displayed by the
larger foundation models."

"For complex zero-shot reasoning tasks ... Orca achieves parity with ChatGPT"

## URL

https://arxiv.org/pdf/2307.02486.pdf

## Primary Organsation

Microsoft

## Authors' abstract

Recent research has focused on enhancing the capability of smaller models
through imitation learning, drawing on the outputs generated by large
foundation models (LFMs). A number of issues impact the quality of these
models, ranging from limited imitation signals from shallow LFM outputs;
small scale homogeneous training data; and most notably a lack of rigorous
evaluation resulting in overestimating the small modelâ€™s capability as they
tend to learn to imitate the style, but not the reasoning process of LFMs. To
address these challenges, we develop Orca, a 13-billion parameter model
that learns to imitate the reasoning process of LFMs. Orca learns from
rich signals from GPT-4 including explanation traces; step-by-step thought
processes; and other complex instructions, guided by teacher assistance from
ChatGPT. To promote this progressive learning, we tap into large-scale and
diverse imitation data with judicious sampling and selection. Orca surpasses
conventional state-of-the-art instruction-tuned models such as Vicuna-13B
by more than 100% in complex zero-shot reasoning benchmarks like Big-
Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity
with ChatGPT on the BBH benchmark and shows competitive performance
(4 pts gap with optimized system message) in professional and academic
examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot
settings without CoT; while trailing behind GPT-4. Our research indicates
that learning from step-by-step explanations, whether these are generated
by humans or more advanced AI models, is a promising direction to improve
model capabilities and skills.
